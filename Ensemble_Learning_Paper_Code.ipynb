{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVu1bSB3LsyA",
        "outputId": "21b5320e-df53-40f5-ac50-9012ee167672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/optimizers/rectified_adam.py:121: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "optimizer = tfa.optimizers.RectifiedAdam(lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5XhfGCGMNbb"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from keras.utils import np_utils\n",
        "import os,cv2\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import  *\n",
        "from tensorflow.keras import layers\n",
        "import gym\n",
        "import random\n",
        "import torch\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten, Dense\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlBsDhCYmCPW",
        "outputId": "79f6ac6e-c9c4-4ed2-f2b2-0881916d9b60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gP1E5FEXgte"
      },
      "outputs": [],
      "source": [
        "# ! unzip \"/content/drive/MyDrive/auglymphnode.zip\" > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j4mAt1fMNVl"
      },
      "outputs": [],
      "source": [
        "# Folder_Path_CT = r\"/content/Dhana & Praneeth/\"\n",
        "\n",
        "# Folder_list_CT = os.listdir(Folder_Path_CT)\n",
        "\n",
        "# img_path_CC = r\"/content/Dhana & Praneeth/malignant selected_3000/\"\n",
        "\n",
        "# img_list_CC = os.listdir(img_path_CC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icBhul5nMNS0"
      },
      "outputs": [],
      "source": [
        "# img_CC_list=[]\n",
        "\n",
        "# for img in img_list_CC:\n",
        "#   input_img = cv2.imread(img_path_CC + img,0)\n",
        "#   input_img = cv2.resize(input_img,(75, 75))\n",
        "#   img_CC_list.append(input_img)\n",
        "\n",
        "# img_CC_array = np.array(img_CC_list)\n",
        "# img_CC_array = img_CC_array.astype('float32')\n",
        "# img_CC_array /= 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8B-ihR0MNRC"
      },
      "outputs": [],
      "source": [
        "# img_path_CN = r\"/content/Dhana & Praneeth/benign selected_3000/\"\n",
        "\n",
        "# img_list_CN = os.listdir(img_path_CN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzNl7_qHMNO3"
      },
      "outputs": [],
      "source": [
        "# img_CN_list=[]\n",
        "\n",
        "# for img in img_list_CN:\n",
        "#   input_img = cv2.imread(img_path_CN + img,0)\n",
        "#   input_img = cv2.resize(input_img,(75, 75))\n",
        "#   img_CN_list.append(input_img)\n",
        "\n",
        "# img_CN_array = np.array(img_CN_list)\n",
        "# img_CN_array = img_CN_array.astype('float32')\n",
        "# img_CN_array /= 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwIIrdM2MNMZ"
      },
      "outputs": [],
      "source": [
        "# X = np.vstack((img_CC_array, img_CN_array))\n",
        "\n",
        "# X = np.expand_dims(X,axis=3)\n",
        "\n",
        "# num_of_samples = img_CC_array.shape[0] + img_CN_array.shape[0]\n",
        "# Y = np.ones((num_of_samples,),dtype='int64')\n",
        "\n",
        "# Y[len(img_CC_array):]=0\n",
        "\n",
        "# X, Y = shuffle(X, Y, random_state=2)\n",
        "# # X = X.reshape(6000, 1*50*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqJ98RDqMNJ7"
      },
      "outputs": [],
      "source": [
        "# print(num_of_samples)\n",
        "# print(X.shape)\n",
        "# print(Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2NuEFUtMNHX"
      },
      "outputs": [],
      "source": [
        "input_shapes = 50, 50, 1\n",
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b2Ba7hNsMrX"
      },
      "outputs": [],
      "source": [
        "# # example of evaluating an adaboost ensemble for classification\n",
        "# from numpy import mean\n",
        "# from numpy import std\n",
        "# from sklearn.datasets import make_classification\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "# from sklearn.ensemble import AdaBoostClassifier\n",
        "# # create the synthetic classification dataset\n",
        "# X, Y = make_classification(random_state=1)\n",
        "# # configure the ensemble model\n",
        "# model = AdaBoostClassifier(n_estimators=50)\n",
        "# # configure the resampling method\n",
        "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# # evaluate the ensemble on the dataset using the resampling method\n",
        "# n_scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# # report ensemble performance\n",
        "# print('Adaboost Ensemble Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAwjGNjhsG2Z"
      },
      "outputs": [],
      "source": [
        "# # example of evaluating a random forest ensemble for classification\n",
        "# from numpy import mean\n",
        "# from numpy import std\n",
        "# from sklearn.datasets import make_classification\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# # create the synthetic classification dataset\n",
        "# X, Y = make_classification(random_state=1)\n",
        "# # configure the ensemble model\n",
        "# model = RandomForestClassifier(n_estimators=50)\n",
        "# # configure the resampling method\n",
        "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# # evaluate the ensemble on the dataset using the resampling method\n",
        "# n_scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# # report ensemble performance\n",
        "# print('Random Forest Ensemble Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eu6ROv8r3zJ"
      },
      "outputs": [],
      "source": [
        "# # example of evaluating a gradient boosting ensemble for classification\n",
        "# from numpy import mean\n",
        "# from numpy import std\n",
        "# from sklearn.datasets import make_classification\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "# from sklearn.ensemble import GradientBoostingClassifier\n",
        "# # create the synthetic classification dataset\n",
        "# X, Y = make_classification(random_state=1)\n",
        "# # configure the ensemble model\n",
        "# model = GradientBoostingClassifier(n_estimators=50)\n",
        "# # configure the resampling method\n",
        "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# # evaluate the ensemble on the dataset using the resampling method\n",
        "# n_scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# # report ensemble performance\n",
        "# print('Gradient Boosting Ensemble Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuFvyyzcsUbj"
      },
      "outputs": [],
      "source": [
        "# # example of evaluating a stacking ensemble for classification\n",
        "# from numpy import mean\n",
        "# from numpy import std\n",
        "# from sklearn.datasets import make_classification\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "# from sklearn.ensemble import StackingClassifier\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# # create the synthetic classification dataset\n",
        "# X, Y = make_classification(random_state=1)\n",
        "# # configure the models to use in the ensemble\n",
        "# models = [('knn', KNeighborsClassifier()), ('tree', DecisionTreeClassifier())]\n",
        "# # configure the ensemble model\n",
        "# model = StackingClassifier(models, final_estimator=LogisticRegression(), cv=3)\n",
        "# # configure the resampling method\n",
        "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# # evaluate the ensemble on the dataset using the resampling method\n",
        "# n_scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# # report ensemble performance\n",
        "# print('Stacking Ensemble Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Dekt3ePNLU1"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "# kf = KFold(n_splits=10,shuffle=True)\n",
        "# kf.get_n_splits()\n",
        "# i1 = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OoZ3UNZNLRc"
      },
      "outputs": [],
      "source": [
        "# train_index_list=[]; test_index_list = []\n",
        "# Xtrain_10fold = []; Ytrain_10fold = []; Xtest_10fold = []; Ytest_10fold = []\n",
        "\n",
        "# for train_index, test_index in kf.split(X,Y):\n",
        "#     train_index_list.append(train_index); test_index_list.append(test_index)\n",
        "#     X_train, X_test = X[train_index], X[test_index]\n",
        "#     Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "#     Xtrain_10fold.append(X_train); Xtest_10fold.append(X_test)\n",
        "#     Ytrain_10fold.append(Y_train); Ytest_10fold.append(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT2so0JhNLPg"
      },
      "outputs": [],
      "source": [
        "# for z in range(10):\n",
        "#     X_train, X_test = Xtrain_10fold[z], Xtest_10fold[z]\n",
        "#     Y_train, Y_test = Ytrain_10fold[z], Ytest_10fold[z]\n",
        "#     Y_train_one_hot = tensorflow.keras.utils.to_categorical(Y_train, 2)\n",
        "\n",
        "#     Y_test_one_hot = tensorflow.keras.utils.to_categorical(Y_test, 2)\n",
        "#     # X_train = X_train.reshape(5400,1*50*50)\n",
        "#     # X_test = X_test.reshape(600,1*50*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX5PrsFYNLM5"
      },
      "outputs": [],
      "source": [
        "# print(X_train.shape)\n",
        "# np.save('/content/drive/MyDrive/EnModels/X_train', X_train)\n",
        "# print(X_test.shape)\n",
        "# np.save('/content/drive/MyDrive/EnModels/X_test', X_test)\n",
        "# print(Y_train.shape)\n",
        "# np.save(r'/content/drive/MyDrive/EnModels/Y_train', Y_train)\n",
        "# print(Y_test.shape)\n",
        "# np.save(r'/content/drive/MyDrive/EnModels/Y_test', Y_test)\n",
        "# print(Y_test_one_hot.shape)\n",
        "# np.save(r'/content/drive/MyDrive/EnModels/Y_test_one_hot', Y_test_one_hot)\n",
        "# print(Y_train_one_hot.shape)\n",
        "# np.save(r'/content/drive/MyDrive/EnModels/Y_train_one_hot', Y_train_one_hot)\n",
        "\n",
        "# X_train = np.load(r'/content/drive/MyDrive/EnModels/X_train.npy')\n",
        "# x_hi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuOvqJ3DcRUM",
        "outputId": "1b6053e5-7d4b-4ec6-a568-05bede2d4ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5100, 50, 50, 1)\n",
            "(900, 50, 50, 1)\n",
            "(5100,)\n",
            "(900,)\n",
            "(900, 2)\n",
            "(5100, 2)\n",
            "(450, 50, 50, 1)\n",
            "(450, 50, 50, 1)\n",
            "(450,)\n",
            "(450,)\n"
          ]
        }
      ],
      "source": [
        "# X_train = np.load(r'/content/drive/MyDrive/EnNewModels/X_train.npy')\n",
        "# X_test = np.load(r'/content/drive/MyDrive/EnNewModels/X_test.npy')\n",
        "# Y_train = np.load(r'/content/drive/MyDrive/EnNewModels/Y_train.npy')\n",
        "# Y_test = np.load(r'/content/drive/MyDrive/EnNewModels/Y_test.npy')\n",
        "# Y_test_one_hot = np.load(r'/content/drive/MyDrive/EnNewModels/Y_test_one_hot.npy')\n",
        "# Y_train_one_hot = np.load(r'/content/drive/MyDrive/EnNewModels/Y_train_one_hot.npy')\n",
        "# X_val = np.load(r'/content/drive/MyDrive/EnNewModels/X_val.npy')\n",
        "# X_testN = np.load(r'/content/drive/MyDrive/EnNewModels/X_testN.npy')\n",
        "# Y_val = np.load(r'/content/drive/MyDrive/EnNewModels/Y_val.npy')\n",
        "# Y_testN = np.load(r'/content/drive/MyDrive/EnNewModels/Y_testN.npy')\n",
        "\n",
        "\n",
        "X_train = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_train.npy')\n",
        "X_test = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_test.npy')\n",
        "Y_train = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_train.npy')\n",
        "Y_test = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_test.npy')\n",
        "Y_test_one_hot = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_test_one_hot.npy')\n",
        "Y_train_one_hot = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_train_one_hot.npy')\n",
        "#450V-450T\n",
        "X_val = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_val.npy')\n",
        "X_testN = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_testN.npy')\n",
        "Y_val = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_val.npy')\n",
        "Y_testN = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_testN.npy')\n",
        "#540V-360T\n",
        "X_val540 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_val540.npy')\n",
        "X_testN360 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_testN360.npy')\n",
        "Y_val540 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_val540.npy')\n",
        "Y_testN360 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_testN360.npy')\n",
        "#360V-540T\n",
        "X_val360 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_val360.npy')\n",
        "X_testN540 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_testN540.npy')\n",
        "Y_val360 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_val360.npy')\n",
        "Y_testN540 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_testN540.npy')\n",
        "#270V-630T\n",
        "X_val270 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_val270.npy')\n",
        "X_testN630 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/X_testN630.npy')\n",
        "Y_val270 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_val270.npy')\n",
        "Y_testN630 = np.load(r'/content/drive/Shareddrives/EN/EnNewModels4/Y_testN630.npy')\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "print(X_test.shape)\n",
        "\n",
        "print(Y_train.shape)\n",
        "\n",
        "print(Y_test.shape)\n",
        "\n",
        "print(Y_test_one_hot.shape)\n",
        "\n",
        "print(Y_train_one_hot.shape)\n",
        "\n",
        "print(X_val.shape)\n",
        "\n",
        "print(X_testN.shape)\n",
        "\n",
        "print(Y_val.shape)\n",
        "\n",
        "print(Y_testN.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riRXA3brCkcT"
      },
      "outputs": [],
      "source": [
        "# X_val270, X_testN630, Y_val270, Y_testN630 = train_test_split(X_test,Y_test ,\n",
        "#                                    random_state=104,\n",
        "#                                    test_size=0.7,\n",
        "#                                    shuffle=False)\n",
        "# np.save('/content/drive/Shareddrives/EN/EnNewModels4/X_val270', X_val270)\n",
        "# np.save('/content/drive/Shareddrives/EN/EnNewModels4/X_testN630', X_testN630)\n",
        "# np.save('/content/drive/Shareddrives/EN/EnNewModels4/Y_val270', Y_val270)\n",
        "# np.save('/content/drive/Shareddrives/EN/EnNewModels4/Y_testN630', Y_testN630)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUUD50Safmo1"
      },
      "outputs": [],
      "source": [
        "# # Create adaboost classifer object\n",
        "# abc = AdaBoostClassifier(n_estimators=100,\n",
        "#                          learning_rate=1)\n",
        "# # Train Adaboost Classifer\n",
        "# model = abc.fit(X_train, Y_train)\n",
        "\n",
        "# #Predict the response for test dataset\n",
        "# Y_pred = model.predict(X_test)\n",
        "\n",
        "# #Adaboost Ensemble Accuracy\n",
        "# print(\"Adaboost Ensemble Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjlNYwCHhHOP"
      },
      "outputs": [],
      "source": [
        "# TN, FP, FN, TP = confusion_matrix(Y_test, Y_pred).ravel()\n",
        "\n",
        "# SEN=TP/(TP+FN)\n",
        "# SPE=TN/(TN+FP)\n",
        "# ACC=(TP+TN)/(TP+TN+FP+FN)\n",
        "\n",
        "# fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred)\n",
        "# AUC = metrics.auc(fpr, tpr)\n",
        "\n",
        "# print(\"SEN\",SEN)\n",
        "# print(\"SPE\",SPE)\n",
        "# print(\"ACC\",ACC*100)\n",
        "# print(\"AUC\",AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0CqhVtQ1KLQ"
      },
      "outputs": [],
      "source": [
        "# dtc = DecisionTreeClassifier(criterion=\"entropy\")\n",
        "\n",
        "# bag_model = BaggingClassifier(base_estimator=dtc, n_estimators=100, bootstrap=True)\n",
        "# bag_model = bag_model.fit(X_train, Y_train)\n",
        "\n",
        "# Y_pred = bag_model.predict(X_test)\n",
        "\n",
        "# #print(bag_model.score(X_test, Y_test)\n",
        "# print(\"Bagging Ensemble Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atirM0nzkG5F"
      },
      "outputs": [],
      "source": [
        "# TN, FP, FN, TP = confusion_matrix(Y_test, Y_pred).ravel()\n",
        "\n",
        "# SEN=TP/(TP+FN)\n",
        "# SPE=TN/(TN+FP)\n",
        "# ACC=(TP+TN)/(TP+TN+FP+FN)\n",
        "\n",
        "# fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred)\n",
        "# AUC = metrics.auc(fpr, tpr)\n",
        "\n",
        "# print(\"SEN\",SEN)\n",
        "# print(\"SPE\",SPE)\n",
        "# print(\"ACC\",ACC*100)\n",
        "# print(\"AUC\",AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60xc9lY2mpWn"
      },
      "outputs": [],
      "source": [
        "# # Create an instance of Random Forest Classifier\n",
        "# #\n",
        "# forest = RandomForestClassifier(criterion='gini',\n",
        "#                                  n_estimators=100,\n",
        "#                                  random_state=1,\n",
        "#                                  n_jobs=2)\n",
        "# #\n",
        "# # Fit the model\n",
        "# #\n",
        "# forest.fit(X_train, Y_train)\n",
        "\n",
        "# #\n",
        "# # Measure model performance\n",
        "# #\n",
        "# Y_pred = forest.predict(X_test)\n",
        "# print('Random Forest Ensemble Accuracy: %.6f' % metrics.accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh-ZlTZZnD4i"
      },
      "outputs": [],
      "source": [
        "# TN, FP, FN, TP = confusion_matrix(Y_test, Y_pred).ravel()\n",
        "\n",
        "# SEN=TP/(TP+FN)\n",
        "# SPE=TN/(TN+FP)\n",
        "# ACC=(TP+TN)/(TP+TN+FP+FN)\n",
        "\n",
        "# fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred)\n",
        "# AUC = metrics.auc(fpr, tpr)\n",
        "\n",
        "# print(\"SEN\",SEN)\n",
        "# print(\"SPE\",SPE)\n",
        "# print(\"ACC\",ACC*100)\n",
        "# print(\"AUC\",AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIQR6-kw0dY-"
      },
      "outputs": [],
      "source": [
        "# gb_clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_features=2, max_depth=2, random_state=0)\n",
        "# gb_clf2.fit(X_train, Y_train)\n",
        "# Y_pred = gb_clf2.predict(X_test)\n",
        "\n",
        "# print('Gradient Boosting Ensemble Accuracy: %.6f' % metrics.accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR4KExhU1ClX"
      },
      "outputs": [],
      "source": [
        "# TN, FP, FN, TP = confusion_matrix(Y_test, Y_pred).ravel()\n",
        "\n",
        "# SEN=TP/(TP+FN)\n",
        "# SPE=TN/(TN+FP)\n",
        "# ACC=(TP+TN)/(TP+TN+FP+FN)\n",
        "\n",
        "# fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred)\n",
        "# AUC = metrics.auc(fpr, tpr)\n",
        "\n",
        "# print(\"SEN\",SEN)\n",
        "# print(\"SPE\",SPE)\n",
        "# print(\"ACC\",ACC*100)\n",
        "# print(\"AUC\",AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyA0oecX3bC8"
      },
      "outputs": [],
      "source": [
        "# # configure the models to use in the ensemble\n",
        "# models = [('knn', KNeighborsClassifier()), ('tree', DecisionTreeClassifier())]\n",
        "# # configure the ensemble model\n",
        "# model = StackingClassifier(models, final_estimator=LogisticRegression(), cv=3)\n",
        "# model.fit(X_train, Y_train)\n",
        "# Y_pred = model.predict(X_test)\n",
        "\n",
        "# print('Stacking Ensemble Accuracy: %.6f' % metrics.accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaGmE6h_3a82"
      },
      "outputs": [],
      "source": [
        "# TN, FP, FN, TP = confusion_matrix(Y_test, Y_pred).ravel()\n",
        "\n",
        "# SEN=TP/(TP+FN)\n",
        "# SPE=TN/(TN+FP)\n",
        "# ACC=(TP+TN)/(TP+TN+FP+FN)\n",
        "\n",
        "# fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred)\n",
        "# AUC = metrics.auc(fpr, tpr)\n",
        "\n",
        "# print(\"SEN\",SEN)\n",
        "# print(\"SPE\",SPE)\n",
        "# print(\"ACC\",ACC*100)\n",
        "# print(\"AUC\",AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-aP-FX0RQZq"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "# from tensorflow.keras.layers import Input\n",
        "\n",
        "# # this could also be the output a different Keras model or layer\n",
        "# input_tensor = Input(shape=(75, 75, 1))\n",
        "\n",
        "# model = InceptionV3(input_tensor=input_tensor, input_shape=input_shapes, weights=None,classes=2, include_top=True)\n",
        "# Incepv3=model\n",
        "# Incepv3.compile(loss ='categorical_crossentropy', optimizer = tfa.optimizers.RectifiedAdam(lr=1e-3),metrics =['acc'])\n",
        "# # Incepv3.summary()\n",
        "# #Try to use RAdam optimiser\n",
        "\n",
        "# # checkpoint\n",
        "# Incepv3.save(r\"/content/\" + \"saved_models/Incepv3.hdf5\")\n",
        "# filepath = \"/content/\" + \"saved_models/Incepv3.hdf5\"\n",
        "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "# callbacks_list = [checkpoint]\n",
        "\n",
        "# #Training the CNN model1\n",
        "# #history = model1.fit(X_train, Y_train, batch_size = 32, epochs = 10, verbose = 1, validation_data = (X_test, Y_test))\n",
        "# history5 = Incepv3.fit(X_train, Y_train_one_hot, batch_size = 10, epochs = 30, callbacks=callbacks_list, verbose = 1, validation_data = (X_test, Y_test_one_hot))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvUuAkOHNLKq"
      },
      "outputs": [],
      "source": [
        "#########################################################\n",
        "#Model 1\n",
        "\n",
        "#Defining the Convolutional Neural Network\n",
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Conv2D(32, (3, 3), input_shape = input_shapes, activation=tensorflow.nn.leaky_relu))\n",
        "model1.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "#model1.add(Dropout(0.2))\n",
        "#Use LeakyReLu instead of ReLU)\n",
        "\n",
        "model1.add(Conv2D(64, (3, 3), activation=tensorflow.nn.leaky_relu))\n",
        "model1.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Conv2D(128, (3, 3), activation=tensorflow.nn.leaky_relu))\n",
        "model1.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model1.add(Dropout(0.4))\n",
        "\n",
        "model1.add(Flatten())\n",
        "\n",
        "model1.add(Dense(50, activation = tensorflow.nn.leaky_relu))\n",
        "model1.add(Dense(100, activation = tensorflow.nn.leaky_relu))\n",
        "\n",
        "model1.add(Dense(2, activation = 'softmax'))\n",
        "\n",
        "\n",
        "#If your targets are one-hot encoded, use categorical_crossentropy. Examples of one-hot encodings:\n",
        "# If your targets are integers, use sparse_categorical_crossentropy.\n",
        "\n",
        "#model1.compile(loss ='sparse_categorical_crossentropy', optimizer='adam', metrics =['acc'])\n",
        "model1.compile(loss ='categorical_crossentropy', optimizer = tfa.optimizers.RectifiedAdam(lr=1e-3),metrics =['acc'])\n",
        "model1.summary()\n",
        "#Try to use RAdam optimiser\n",
        "\n",
        "# checkpoint\n",
        "model1.save(r\"/content/\" + \"saved_models/model1.hdf5\")\n",
        "filepath = \"/content/\" + \"saved_models/model1.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#Training the CNN model1\n",
        "#history = model1.fit(X_train, Y_train, batch_size = 32, epochs = 10, verbose = 1, validation_data = (X_test, Y_test))\n",
        "history1 = model1.fit(X_train, Y_train_one_hot, batch_size = 10, epochs = 30, callbacks=callbacks_list, verbose = 1, validation_data = (X_test, Y_test_one_hot))\n",
        "\n",
        "#model1.save('saved_models/model1.hdf5')\n",
        "#model1.save(r\"/content/\" + \"saved_models/model1.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j8SPVWBNWQF"
      },
      "outputs": [],
      "source": [
        "##########################################################\n",
        "#Model2\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Conv2D(32, (3, 3), input_shape = input_shapes, activation=tensorflow.nn.leaky_relu))\n",
        "model2.add(Conv2D(32, (3, 3), activation=tensorflow.nn.leaky_relu))\n",
        "model2.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "model2.add(Conv2D(64, (3, 3), activation=tensorflow.nn.leaky_relu))\n",
        "model2.add(Conv2D(64, (3, 3), activation=tensorflow.nn.leaky_relu))\n",
        "model2.add(Conv2D(64, (3, 3), activation=tensorflow.nn.leaky_relu))\n",
        "model2.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "model2.add(Conv2D(128, (3, 3), activation=tensorflow.nn.leaky_relu))\n",
        "model2.add(Conv2D(128, (3,3)))\n",
        "model2.add(Conv2D(128, (3,3)))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(50,))\n",
        "model2.add(Dense(100,))\n",
        "model2.add(Dense(2, activation = 'softmax'))\n",
        "\n",
        "model2.compile(loss ='categorical_crossentropy', optimizer = tfa.optimizers.RectifiedAdam(lr=1e-3), metrics =['acc'])\n",
        "model2.summary()\n",
        "\n",
        "# checkpoint\n",
        "model2.save(r\"/content/\" + \"saved_models/model2.hdf5\")\n",
        "filepath = \"/content/\" + \"saved_models/model1.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history2 = model2.fit(X_train, Y_train_one_hot, batch_size = 10, epochs = 30, callbacks=callbacks_list, verbose = 1, validation_data = (X_test, Y_test_one_hot))\n",
        "\n",
        "#model2.save('saved_models/model2.hdf5')\n",
        "#model2.save(r\"/content/\" + \"saved_models/model2.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHfVZDslNWD_"
      },
      "outputs": [],
      "source": [
        "###################################################################\n",
        "#Model 3 Inception Net\n",
        "#\n",
        "#model3 = Sequential()\n",
        "from keras.layers import Input\n",
        "input_img = Input(input_shapes)\n",
        "\n",
        "tower_1 = Conv2D(64, (1,1), padding='same', activation=tensorflow.nn.leaky_relu)(input_img)\n",
        "tower_1 = Conv2D(64, (3,3), padding='same', activation=tensorflow.nn.leaky_relu)(tower_1)\n",
        "tower_2 = Conv2D(64, (1,1), padding='same', activation=tensorflow.nn.leaky_relu)(input_img)\n",
        "tower_2 = Conv2D(64, (5,5), padding='same', activation=tensorflow.nn.leaky_relu)(tower_2)\n",
        "tower_3 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n",
        "tower_3 = Conv2D(64, (1,1), padding='same', activation=tensorflow.nn.leaky_relu)(tower_3)\n",
        "\n",
        "output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis = 3)\n",
        "output = Flatten()(output)\n",
        "output = Dense(50,)(output)\n",
        "output = Dense(100,)(output)\n",
        "out    = Dense(2, activation='softmax')(output)\n",
        "\n",
        "model3 = Model(inputs = input_img, outputs = out)\n",
        "\n",
        "#If your targets are one-hot encoded, use categorical_crossentropy. Examples of one-hot encodings:\n",
        "# If your targets are integers, use sparse_categorical_crossentropy.\n",
        "\n",
        "#model1.compile(loss ='sparse_categorical_crossentropy', optimizer='adam', metrics =['acc'])\n",
        "model3.compile(loss ='categorical_crossentropy', optimizer = tfa.optimizers.RectifiedAdam(lr=1e-3), metrics =['acc'])\n",
        "model3.summary()\n",
        "\n",
        "# checkpoint\n",
        "model3.save(r\"/content/\" + \"saved_models/model3.hdf5\")\n",
        "filepath = \"/content/\" + \"saved_models/model1.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#Training the CNN model1\n",
        "#history = model1.fit(X_train, y_train, batch_size = 128, epochs = 10, verbose = 1, validation_data = (X_test, y_test))\n",
        "history3 = model3.fit(X_train, Y_train_one_hot, batch_size = 10, epochs = 30, callbacks=callbacks_list, verbose = 1, validation_data = (X_test, Y_test_one_hot))\n",
        "\n",
        "#model3.save('saved_models/model3.hdf5')\n",
        "#model3.save(r\"/content/\" + \"saved_models/model3.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyIImNEXdX5y"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32,(2,2),input_shape=input_shapes,activation='linear'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=.1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(32, (2,2),activation='linear'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=.1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(64, (2,2),activation='linear'))\n",
        "model.add(LeakyReLU(alpha=.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (2,2),activation='linear'))\n",
        "model.add(LeakyReLU(alpha=.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (2,2),activation='linear'))\n",
        "model.add(LeakyReLU(alpha=.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=.1))\n",
        "model.add(Conv2D(2,(1,1)))\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# for z in range(10):\n",
        "#     X_train, X_test = Xtrain_10fold[z], Xtest_10fold[z]\n",
        "#     Y_train, Y_test = Ytrain_10fold[z], Ytest_10fold[z]\n",
        "#     Y_train_one_hot = tensorflow.keras.utils.to_categorical(Y_train, 2)\n",
        "\n",
        "#     Y_test_one_hot = tensorflow.keras.utils.to_categorical(Y_test, 2)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "      optimizer=tfa.optimizers.RectifiedAdam(lr=1e-3),\n",
        "      metrics=['acc'])\n",
        "# model.summary()\n",
        "\n",
        "model.save(r\"/content/\" + \"saved_models/model4.hdf5\")\n",
        "filepath = \"/content/\" + \"saved_models/model4.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history4=model.fit(X_train, Y_train_one_hot,\n",
        "      batch_size=10,\n",
        "      # validation_split=0.0,\n",
        "      epochs=30,\n",
        "      validation_data = (X_test, Y_test_one_hot),\n",
        "      verbose=1,callbacks=callbacks_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZxpIK475Af2"
      },
      "outputs": [],
      "source": [
        "AlexNet = Sequential()\n",
        "\n",
        "#1st Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=96, input_shape=(50,50,1), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#2nd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#3rd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#4th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#5th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#Passing it to a Fully Connected layer\n",
        "AlexNet.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "AlexNet.add(Dense(4096, input_shape=(50,50,1)))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#2nd Fully Connected Layer\n",
        "AlexNet.add(Dense(4096))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#3rd Fully Connected Layer\n",
        "AlexNet.add(Dense(1000))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#Output Layer\n",
        "AlexNet.add(Dense(2))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('softmax'))\n",
        "\n",
        "#Model Summary\n",
        "AlexNet.summary()\n",
        "\n",
        "model1 = AlexNet\n",
        "\n",
        "model1.summary()\n",
        "\n",
        "epochs = 2\n",
        "opt = Adamax()\n",
        "\n",
        "AlexNet.compile(loss ='categorical_crossentropy', optimizer = tfa.optimizers.RectifiedAdam(lr=1e-3),metrics =['acc'])\n",
        "AlexNet.summary()\n",
        "#Try to use RAdam optimiser\n",
        "\n",
        "# checkpoint\n",
        "AlexNet.save(r\"/content/\" + \"saved_models/model5.hdf5\")\n",
        "filepath = \"/content/\" + \"saved_models/model5.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#Training the CNN model1\n",
        "#history = model1.fit(X_train, Y_train, batch_size = 32, epochs = 10, verbose = 1, validation_data = (X_test, Y_test))\n",
        "history5 = model1.fit(X_train, Y_train_one_hot, batch_size = 10, epochs = 30, callbacks=callbacks_list, verbose = 1, validation_data = (X_test, Y_test_one_hot))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7weuX9vHhE0",
        "outputId": "84eaa927-8ced-464b-df33-1bdc6ea3d194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENSEMBLE ACCURACY: 0.9433333333333334\n",
            "SEN 0.925\n",
            "SPE 0.9608695652173913\n",
            "ACC 94.33333333333334\n",
            "AUC 0.9429347826086957\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "ROOT_DIR = '/content/drive/Shareddrives/EN/EnNewModels4/'\n",
        "\n",
        "def majority_voting(Alex_prediction, Fcn_prediction,Resv2_prediction,Xcep_prediction,c_model1_prediction,c_model2_prediction,c_model3_prediction):\n",
        "    \"\"\" Find the majority vote among the predictions of the given models\n",
        "\n",
        "    Args:\n",
        "        rf_prediction (numpy array): predictions of random forest model.\n",
        "\n",
        "        sv_prediction (numpy array): predictions of SVM model.\n",
        "\n",
        "        custom_prediction (numpy array): predictions of custom CNN model.\n",
        "\n",
        "        vgg_prediction (numpy array): predictions of CNN-VGG16 model.\n",
        "\n",
        "    Returns:\n",
        "        (numpy array): predictions of ensemble-majority voting model\n",
        "\n",
        "    \"\"\"\n",
        "    # loop over all predictions\n",
        "    final_prediction = list()\n",
        "    for AlexT, FcnT, XcepT, Resv2T,C_model1T,C_model2T,C_model3T in zip(Alex_prediction,\n",
        "                                           Fcn_prediction,\n",
        "                                           Resv2_prediction,\n",
        "                                           Xcep_prediction,\n",
        "                                           c_model1_prediction,\n",
        "                                           c_model2_prediction,\n",
        "                                           c_model3_prediction):\n",
        "        # Keep track of votes per class\n",
        "        benign = malignant = 0\n",
        "\n",
        "        # Loop over all models\n",
        "        image_predictions = [AlexT, FcnT, XcepT, Resv2T,C_model1T,C_model2T,C_model3T]\n",
        "        for img_prediction in image_predictions:\n",
        "            # Voting\n",
        "            if img_prediction == 'benign':\n",
        "                benign += 1\n",
        "            elif img_prediction == 'malignant':\n",
        "                malignant += 1\n",
        "\n",
        "        # Find max vote\n",
        "        count_dict = {'bening': benign, 'malignant': malignant}\n",
        "        highest = max(count_dict.values())\n",
        "        max_values = [k for k, v in count_dict.items() if v == highest]\n",
        "        ensemble_prediction = []\n",
        "        for max_value in max_values:\n",
        "            if max_value == 'bening':\n",
        "                ensemble_prediction.append('bening')\n",
        "            elif max_value == 'malignant':\n",
        "                ensemble_prediction.append('malignant')\n",
        "\n",
        "        predict = ''\n",
        "        if len(ensemble_prediction) > 1:\n",
        "            predict = AlexT\n",
        "        else:\n",
        "            predict = ensemble_prediction[0]\n",
        "\n",
        "        # Store max vote\n",
        "        final_prediction.append(predict)\n",
        "\n",
        "    return np.array(final_prediction)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\" Load data.\n",
        "    Normalize and encode.\n",
        "    Train ensemble-majority voting model.\n",
        "    Print accuracy of the model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        Alex = load_model(os.path.join(ROOT_DIR, 'Alex-4.hdf5'))\n",
        "        Fcn = load_model(os.path.join(ROOT_DIR, 'fcn-4-3.hdf5'))\n",
        "        Resv2 = load_model(os.path.join(ROOT_DIR, 'ResV2-4.hdf5'))\n",
        "        Xcep = load_model(os.path.join(ROOT_DIR, 'XcepEdit-4.hdf5'))\n",
        "        C_model1 = load_model(os.path.join(ROOT_DIR, 'model1-4-1.hdf5'))\n",
        "        C_model2 = load_model(os.path.join(ROOT_DIR, 'model2-4-1.hdf5'))\n",
        "        C_model3 = load_model(os.path.join(ROOT_DIR, 'model3-4.hdf5'))\n",
        "        # Normalize image for CNN\n",
        "\n",
        "        Alex_prediction = np.argmax(Alex.predict(X_test), axis=-1)\n",
        "        #custom_prediction = labeler.inverse_transform(custom_prediction)\n",
        "        Fcn_prediction = np.argmax(Fcn.predict(X_test), axis=-1)\n",
        "        #custom_prediction = labeler.inverse_transform(custom_prediction)\n",
        "        Resv2_prediction = np.argmax(Resv2.predict(X_test), axis=-1)\n",
        "        #inception_prediction = labeler.inverse_transform(inception_prediction)\n",
        "        Xcep_prediction = np.argmax(Xcep.predict(X_test), axis=-1)\n",
        "        c_model1_prediction = np.argmax(C_model1.predict(X_test), axis=-1)\n",
        "        c_model2_prediction = np.argmax(C_model2.predict(X_test), axis=-1)\n",
        "        c_model3_prediction = np.argmax(C_model3.predict(X_test), axis=-1)\n",
        "\n",
        "        final_prediction = majority_voting(Alex_prediction,\n",
        "                                           Fcn_prediction,\n",
        "                                           Resv2_prediction,\n",
        "                                           Xcep_prediction,\n",
        "                                           c_model1_prediction,\n",
        "                                           c_model2_prediction,\n",
        "                                           c_model3_prediction)\n",
        "\n",
        "        # Compute accuracy\n",
        "        print(\"ENSEMBLE ACCURACY:\", accuracy_score(Y_test, final_prediction))\n",
        "\n",
        "        # Save model\n",
        "        np.save(os.path.join(ROOT_DIR, 'Ensemble.npy'), final_prediction)\n",
        "\n",
        "    except FileNotFoundError as err:\n",
        "        print('[ERROR] CNN-custom '\n",
        "              'and VGG16 models before executing ensemble model!')\n",
        "        print('[ERROR MESSAGE]', err)\n",
        "\n",
        "    TN, FP, FN, TP = confusion_matrix(Y_test, final_prediction).ravel()\n",
        "\n",
        "    SEN=TP/(TP+FN)\n",
        "    SPE=TN/(TN+FP)\n",
        "    ACC=(TP+TN)/(TP+TN+FP+FN)\n",
        "\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, final_prediction)\n",
        "    AUC = metrics.auc(fpr, tpr)\n",
        "\n",
        "    print(\"SEN\",SEN)\n",
        "    print(\"SPE\",SPE)\n",
        "    print(\"ACC\",ACC*100)\n",
        "    print(\"AUC\",AUC)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UaXAJ9Bdpz1"
      },
      "outputs": [],
      "source": [
        "# # load models from file\n",
        "# def load_all_models(n_models):\n",
        "# \tall_models = list()\n",
        "# \tfor i in range(n_models):\n",
        "# \t\t# define filename for this ensemble\n",
        "# \t\tfilename = '/content/drive/MyDrive/EnModels/model' + str(i + 1) + '.hdf5'\n",
        "# \t\t# load model from file\n",
        "# \t\tmodel = load_model(filename,)\n",
        "# \t\t# add to list of members\n",
        "# \t\tall_models.append(model)\n",
        "# \t\tprint('>loaded %s' % filename)\n",
        "# \treturn all_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKt_KhAzaT-s"
      },
      "outputs": [],
      "source": [
        "# load models from file\n",
        "from keras.models import load_model\n",
        "def load_all_models(n_models):\n",
        "  all_models = list()\n",
        "  # for i in range(n_models):\n",
        "  # define filename for this ensemble\n",
        "  # filename = '/content/drive/MyDrive/EnModels/model' + str(i + 4) + '.hdf5'\n",
        "  filename1 = '/content/drive/Shareddrives/EN/EnNewModels4/Alex-4.hdf5'\n",
        "  filename2 = '/content/drive/Shareddrives/EN/EnNewModels4/fcn-4-3.hdf5'\n",
        "  filename3 = '/content/drive/Shareddrives/EN/EnNewModels4/model3-4.hdf5'\n",
        "  filename4 = '/content/drive/Shareddrives/EN/EnNewModels4/XcepEdit-4.hdf5'\n",
        "  filename5 = '/content/drive/Shareddrives/EN/EnNewModels4/ResV2-4.hdf5'\n",
        "  filename6 = '/content/drive/Shareddrives/EN/EnNewModels4/model2-4-1.hdf5'\n",
        "  filename7 = '/content/drive/Shareddrives/EN/EnNewModels4/model1-4-1.hdf5'\n",
        "  filename8 = '/content/drive/Shareddrives/EN/EnNewModels4/model4-4.hdf5'\n",
        "\n",
        "  # load model from file\n",
        "  model1 = load_model(filename1)\n",
        "  model2 = load_model(filename2)\n",
        "  model3 = load_model(filename3)\n",
        "  model4 = load_model(filename4)\n",
        "  model5 = load_model(filename5)\n",
        "  model6 = load_model(filename6)\n",
        "  model7 = load_model(filename7)\n",
        "  model8 = load_model(filename8)\n",
        "  # add to list of members\n",
        "  # all_models.append(model1)\n",
        "  # all_models.append(model2)\n",
        "  all_models.append(model3)\n",
        "  all_models.append(model4)\n",
        "  # all_models.append(model5)\n",
        "  # all_models.append(model6)\n",
        "  all_models.append(model7)\n",
        "  all_models.append(model8)\n",
        "  # print('>loaded %s' % filename)\n",
        "  print(\"Files loaded\")\n",
        "  return all_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9nDT_gVdywo",
        "outputId": "9302c070-fd50-468f-a462-20348f182323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files loaded\n",
            "Loaded 4 models\n"
          ]
        }
      ],
      "source": [
        "n_members = 5\n",
        "members = load_all_models(n_members)\n",
        "print('Loaded %d models' % len(members))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx9Rmk4tgnVA"
      },
      "outputs": [],
      "source": [
        "from numpy import dstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-_4s83EdyzN"
      },
      "outputs": [],
      "source": [
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX):\n",
        "\tstackX = None\n",
        "\tfor model in members:\n",
        "\t\t# make prediction\n",
        "\t\tyhat = model.predict(inputX, verbose=0)\n",
        "\t\t# stack predictions into [rows, members, probabilities]\n",
        "\t\tif stackX is None:\n",
        "\t\t\tstackX = yhat #\n",
        "\t\telse:\n",
        "\t\t\tstackX = dstack((stackX, yhat))\n",
        "\t# flatten predictions to [rows, members x probabilities]\n",
        "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "\treturn stackX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmGHU-wLdy19"
      },
      "outputs": [],
      "source": [
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model(members, inputX, inputy):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# fit standalone model\n",
        "\tmodel = LogisticRegression() #meta learner\n",
        "\tmodel.fit(stackedX, inputy)\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkWBmgv5d5n4"
      },
      "outputs": [],
      "source": [
        "model = fit_stacked_model(members, X_val540,Y_val540)\n",
        "# model = fit_stacked_model(members, X_val,Y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4F0PxTOd5p_"
      },
      "outputs": [],
      "source": [
        "# make a prediction with the stacked model\n",
        "def stacked_prediction(members, model, inputX):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# make a prediction\n",
        "\t# model.save(r\"/content/drive/Shareddrives/EN/EnNewModels4/\" + \"stacked2.hdf5\")\n",
        "\tyhat = model.predict(stackedX)\n",
        "\treturn yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clUTnCjqd5sv"
      },
      "outputs": [],
      "source": [
        "# evaluate model on test set\n",
        "yhat = stacked_prediction(members, model, X_testN360)\n",
        "# yhat = stacked_prediction(members, model, X_testN)\n",
        "# score = val_acc(y_test/1.0, yhat/1.0)\n",
        "# print('Stacked F Score:', score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REA-gnXQd5vs",
        "outputId": "20737da2-92dc-4743-ebe6-56e9782951d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SEN 0.9884393063583815\n",
            "SPE 0.9893048128342246\n",
            "ACC 98.88888888888889\n",
            "AUC 0.988872059596303\n"
          ]
        }
      ],
      "source": [
        "TN, FP, FN, TP = confusion_matrix(Y_testN360, yhat).ravel()\n",
        "\n",
        "SEN=TP/(TP+FN)\n",
        "SPE=TN/(TN+FP)\n",
        "ACC=(TP+TN)/(TP+TN+FP+FN)\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(Y_testN360, yhat)\n",
        "AUC = metrics.auc(fpr, tpr)\n",
        "\n",
        "print(\"SEN\",SEN)\n",
        "print(\"SPE\",SPE)\n",
        "print(\"ACC\",ACC*100)\n",
        "print(\"AUC\",AUC)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
